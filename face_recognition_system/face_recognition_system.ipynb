{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec7d143",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'facenet_pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#!pip install facenet_pytorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfacenet_pytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MTCNN\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'facenet_pytorch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#!pip install facenet_pytorch\n",
    "from facenet_pytorch import MTCNN\n",
    " \n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import pandas as pd\n",
    " \n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2 \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset \n",
    "from torchvision.models import resnet50 \n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import   DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38b4e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDetector:\n",
    "    def __init__(self, weights_path, device=None):\n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = device\n",
    "        self.model = MTCNN(keep_all=True, device=self.device)\n",
    "        self.load_weights(weights_path)\n",
    "\n",
    "    def load_weights(self, weights_path):\n",
    "        # Загрузка весов в модель\n",
    "        state_dict = torch.load(weights_path, map_location=self.device)\n",
    "        self.model.load_state_dict(state_dict)\n",
    "\n",
    "    def detect_faces(self, image):        \n",
    "        boxes, _ = self.model.detect(image)\n",
    "        boxes = boxes.astype(int)\n",
    "\n",
    "        # Делаю padding\n",
    "        boxes[:, 0] -= 10\n",
    "        boxes[:, 1] -= 10\n",
    "        boxes[:, 2] += 20\n",
    "        boxes[:, 3] += 20\n",
    "\n",
    "        return boxes\n",
    "\n",
    "    def box_faces(self, image):\n",
    "        boxes = self.detect_faces(image)\n",
    "       \n",
    "        img = Image.fromarray(img.astype('uint8'), 'RGB')\n",
    "        result = []\n",
    "\n",
    "        for box in boxes:\n",
    "            x_left, y_left = box[0], box[1]\n",
    "            x_right, y_right = box[2], box[3]\n",
    "\n",
    "            face_area = img.crop((x_left, y_left, x_right, y_right))\n",
    "            face_area = np.array(face_area, dtype='uint8')\n",
    "            result.append(face_area)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def display_faces(self, image):\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        boxes = self.detect_faces(image)\n",
    "        image_np = np.array(image)\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(image_np)\n",
    "\n",
    "        if boxes is not None:\n",
    "            for box in boxes:\n",
    "                rect = plt.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], fill=False, color='red')\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beb8b7da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSimpleCNNModel\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule) :\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class SimpleCNNModel(nn.Module) :\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = resnet50(pretrained=True)\n",
    "        self.model.fc = nn.Linear(2048, 68*2)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9a5434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer():\n",
    "    def __init__(self, model, img):\n",
    "        self.model = model\n",
    "        self.transform = A.Compose([\n",
    "            A.Resize(220, 220),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "        self.img = self.transform(image=img)['image']\n",
    "\n",
    "\n",
    "    def coord_landmarks(self):\n",
    "        predict = self.model(torch.tensor(np.expand_dims(self.img, axis=0)).float())\n",
    "\n",
    "        num_landmarks = [i for i in range(16, -1, -1)] + [i for i in range(17, 20)] + [i for i in range(24, 27)] + [16]\n",
    "\n",
    "        x_pred = predict.detach().numpy()[0, ::2][num_landmarks]\n",
    "        y_pred = predict.detach().numpy()[0, 1::2][num_landmarks]\n",
    "\n",
    "        x_pred = x_pred.reshape(-1, 1)\n",
    "        y_pred = y_pred.reshape(-1, 1)\n",
    "\n",
    "        coords = np.concatenate((x_pred, y_pred), axis=1)\n",
    "\n",
    "        return coords\n",
    "\n",
    "\n",
    "    def show_landmarks(self):\n",
    "        coords = self.coord_landmarks()\n",
    "        x = coords[:, 0]\n",
    "        y = coords[:, 1]\n",
    "        size_points = 10\n",
    "\n",
    "        img = self.img.numpy()[0]\n",
    "\n",
    "        plt.scatter(x, y, s=size_points)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "\n",
    "\n",
    "    def normalizer(self):\n",
    "        coords = self.coord_landmarks().astype(int)\n",
    "        self.img = self.img.numpy()[0]\n",
    "\n",
    "        mask = np.zeros((self.img.shape[0], self.img.shape[1]))\n",
    "        mask = cv2.fillConvexPoly(mask, coords, (255, 255, 255))\n",
    "        mask = mask.astype(bool)\n",
    "\n",
    "        out = np.zeros_like(self.img)\n",
    "        out[mask] = self.img[mask]\n",
    "\n",
    "        plt.imshow(out, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c62319c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extractor:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def transform_image(self, image):\n",
    "        transform = transforms.Compose([transforms.Resize((128, 128)),transforms.ToTensor(),transforms.Grayscale(),])\n",
    "        \n",
    "        image = transform(image)\n",
    "     \n",
    "        image = image.unsqueeze(0)\n",
    "        return image\n",
    "         \n",
    "         \n",
    "    def get_features(self, image):\n",
    "        image = self.transform_image(image)    \n",
    "        \n",
    "        features = model(image)         \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef181711",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Check_similarity():\n",
    "    def __init__(self, image_path1, image_path2):\n",
    "        self.image1 = cv2.imread(image_path1)\n",
    "        self.image2 = cv2.imread(image_path2)         \n",
    "        \n",
    "    \n",
    "    def detect(self):\n",
    "        detector = FaceDetector()\n",
    "        face1 = detector.box_faces(self.image1)\n",
    "        face2 = detector.box_faces(self.image2)\n",
    "        return face1, face2\n",
    "    \n",
    "    def normalization(self):\n",
    "        face1, face2 = self.detect() \n",
    "        model = torch.load('\\model_normalizer')\n",
    "        normalizer1 = Normalizer(model, face1)\n",
    "        normalizer2 = Normalizer(model, face2)\n",
    "        norm1 = normalizer1.noramlizer()\n",
    "        norm2 = normalizer2.noramlizer()\n",
    "        return norm1, norm2\n",
    "    \n",
    "    def extract(self):\n",
    "        face1, face2 = self.normalization()\n",
    "        model = torch.load('\\model1.pth')\n",
    "        \n",
    "        extractor = Extractor(model)\n",
    "        features1 = extractor.get_features(face1)\n",
    "        features2 = extractor.get_features(face2)\n",
    "        \n",
    "        return features1, features2\n",
    "    \n",
    "    \n",
    "    def similarity_score(self):    \n",
    "        features1, features2 = self.extract()\n",
    "        euclidean_dist = torch.norm(features1 - features2, p=2) \n",
    "        similarity = torch.exp(-euclidean_dist)\n",
    "    \n",
    "        return similarity "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
